{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919d787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer_encoder import Exp_Informer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d5c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "# args.data = 'ETTh1' # data\n",
    "args.data = 'custom_v2' # data\n",
    "args.root_path = './data' # root path of data file\n",
    "args.data_path = 'data/' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'OT' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 600 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 38 # encoder input size\n",
    "args.dec_in = 7 # decoder input size\n",
    "args.c_out = 7 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 16 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 6\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = [0,1,2,3]\n",
    "\n",
    "args.use_multi_gpu = True\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6103a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10512a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12bde13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom_v2', 'root_path': './data', 'data_path': 'data/', 'features': 'MS', 'target': 'OT', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 600, 'label_len': 48, 'pred_len': 24, 'enc_in': 38, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 16, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 6, 'patience': 3, 'des': 'exp', 'use_gpu': False, 'gpu': [0, 1, 2, 3], 'use_multi_gpu': True, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cedcf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa106753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "exp = Exp(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf721173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ETDataset' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/zhouhaoyi/ETDataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0d6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 160\n"
     ]
    }
   ],
   "source": [
    "train_data, train_loader = exp._get_data(flag = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b22bfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<data.data_loader.Dataset_Custom_v2 at 0x15807af40>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x15807a6a0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6bbefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6df4f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33-146'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys_train[159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8962857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1fa0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.41498589515686035 torch.Size([16, 600, 38]) torch.Size([16])\n",
      "1 0.0065898895263671875 torch.Size([16, 600, 38]) torch.Size([16])\n",
      "2 0.006454944610595703 torch.Size([16, 600, 38]) torch.Size([16])\n",
      "3 0.006295919418334961 torch.Size([16, 600, 38]) torch.Size([16])\n",
      "4 0.006285905838012695 torch.Size([16, 600, 38]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "t = time.time()\n",
    "for i,(x,y) in enumerate(train_loader):\n",
    "    if (i==5):\n",
    "        break\n",
    "    print(i, time.time()-t,x.shape,y.shape)\n",
    "    t = time.time()\n",
    "    xx = x.float()\n",
    "    yy = y.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a6e7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = exp.model(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10416df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2449,  0.1217,  1.6161,  0.0169,  0.1973,  1.9635,  2.7596, -2.9397,\n",
       "         0.1978, -0.2090, -0.6457, -0.1794,  0.2553,  0.2599,  1.0676, -0.9632],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe7edbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06a85376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 600, 38])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2975158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 1\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "network = exp.model\n",
    "optimizer = optim.Adam(network.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        target = target.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        torch.save(network.state_dict(), './model.pth')\n",
    "        torch.save(optimizer.state_dict(), './optimizer.pth')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [144/160 (90%)]\tLoss: 0.741468\n",
      "Train Epoch: 2 [144/160 (90%)]\tLoss: 0.820529\n",
      "Train Epoch: 3 [144/160 (90%)]\tLoss: 1.416993\n",
      "Train Epoch: 4 [144/160 (90%)]\tLoss: 0.705203\n",
      "Train Epoch: 5 [144/160 (90%)]\tLoss: 0.580442\n",
      "Train Epoch: 6 [144/160 (90%)]\tLoss: 0.960187\n",
      "Train Epoch: 7 [144/160 (90%)]\tLoss: 1.516874\n",
      "Train Epoch: 8 [144/160 (90%)]\tLoss: 0.638392\n",
      "Train Epoch: 9 [144/160 (90%)]\tLoss: 0.310821\n",
      "Train Epoch: 10 [144/160 (90%)]\tLoss: 0.767439\n",
      "Train Epoch: 11 [144/160 (90%)]\tLoss: 0.594751\n",
      "Train Epoch: 12 [144/160 (90%)]\tLoss: 0.720503\n",
      "Train Epoch: 13 [144/160 (90%)]\tLoss: 1.510111\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "# args.data = 'ETTh1' # data\n",
    "args.data = 'custom' # data\n",
    "args.root_path = '../optiver-realized-volatility-prediction/' # root path of data file\n",
    "args.data_path = 'data' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'OT' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 96 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 7 # encoder input size\n",
    "args.dec_in = 7 # decoder input size\n",
    "args.c_out = 7 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 6\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = [0,1,2,3]\n",
    "\n",
    "args.use_multi_gpu = True\n",
    "args.devices = '0,1,2,3'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
